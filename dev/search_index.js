var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = BytePairEncoding","category":"page"},{"location":"#BytePairEncoding.jl","page":"Home","title":"BytePairEncoding.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pure Julia implementation of the Byte Pair Encoding(BPE) method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [BytePairEncoding]","category":"page"},{"location":"#BytePairEncoding.BPELearner","page":"Home","title":"BytePairEncoding.BPELearner","text":"BPELearner(tokenization::AbstractTokenization; min_freq = 10, endsym = \"</w>\", sepsym = nothing)\n\nConstruct a learner with a tokenization which has BPETokenization and NoBPE inside.\n\n(bper::BPELearner)(word_counts, n_merge)\n\nCalling the learner on a word_counts dictionary (created by count_words) generate a new tokenization   where NoBPE is replaced with the learned BPE.\n\n\n\n\n\n","category":"type"},{"location":"#BytePairEncoding.bbpe2tiktoken","page":"Home","title":"BytePairEncoding.bbpe2tiktoken","text":"bbpe2tiktoken(tkr)\n\nConvert a gpt2-like byte-level tokenizer (with bpe::BPE) to tiktoken tokenizer (with bpe::TikToken).  If there is a CodeNormalizer in the tokenizer, it will be removed accordingly.\n\nsee also: tiktoken2bbpe\n\n\n\n\n\n","category":"function"},{"location":"#BytePairEncoding.count_words-Tuple{BPELearner, AbstractVector}","page":"Home","title":"BytePairEncoding.count_words","text":"count_words(bper::BPELearner, files::AbstractVector)\n\nGiven a list of files (where each line of the file would be considered as a (multi-sentences) document).   Tokenize those file a count the occurence of each word token.\n\n\n\n\n\n","category":"method"},{"location":"#BytePairEncoding.gpt2_codemap-Tuple{}","page":"Home","title":"BytePairEncoding.gpt2_codemap","text":"the codemap used by openai gpt2\n\n\n\n\n\n","category":"method"},{"location":"#BytePairEncoding.gpt2_tokenizer-Tuple{Any}","page":"Home","title":"BytePairEncoding.gpt2_tokenizer","text":"the tokenizer used by openai gpt2\n\n\n\n\n\n","category":"method"},{"location":"#BytePairEncoding.load_gpt2-Tuple{}","page":"Home","title":"BytePairEncoding.load_gpt2","text":"load_gpt2()\n\nLoad gpt2 tokenizer.\n\n\n\n\n\n","category":"method"},{"location":"#BytePairEncoding.load_tiktoken-Tuple{Any}","page":"Home","title":"BytePairEncoding.load_tiktoken","text":"load_tiktoken(name)\n\nLoad tiktoken tokenizer. name can be \"cl100k_base\", \"p50k_base\", \"p50k_base\", \"r50k_base\", or \"gpt2\".\n\n\n\n\n\n","category":"method"},{"location":"#BytePairEncoding.tiktoken2bbpe","page":"Home","title":"BytePairEncoding.tiktoken2bbpe","text":"tiktoken2bbpe(tkr, codemap::Union{CodeMap, Nothing} = nothing)\n\nConvert a tiktoken tokenizer (with bpe::TikToken) to gpt2-like byte-level tokenizer (with bpe::BPE).  If codemap is provided, it will add the corresponding CodeNormalizer to the tokenizer.\n\nsee also: bbpe2tiktoken\n\n\n\n\n\n","category":"function"}]
}
