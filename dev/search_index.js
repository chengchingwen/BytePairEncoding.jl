var documenterSearchIndex = {"docs":
[{"location":"learn/#Learning-BPE","page":"Learning BPE","title":"Learning BPE","text":"","category":"section"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"We also provide the functionality to learn new BPE map accordingly.","category":"page"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"# create a bpe we want to learn\njulia> mybpe = GenericBPE{String}(; oldendsym = \"@w@\", sepsym = \"=\",\n\tinput_transform = BytePairEncoding.gpt2_tokenizer, codemap = BytePairEncoding.default_codemap(),\n\tnormalizer = BytePairEncoding.UtfNormalizer(:NFKC_CF), glossaries = BytePairEncoding.Glossary([r\"[0-9]\"]))\nGenericBPE{String}(n_merge=0, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ']), normalizer=UtfNormalizer(1038))\n\n# create the learner, here we set the maximum merge rank = 5000, and required minimum word frequency = 10\njulia> bper = BPELearner(mybpe, 5000, 10)\nBPELearner(bpe = GenericBPE{String}(n_merge=0, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ']), normalizer=UtfNormalizer(1038)), merge = 5000, min_freq = 10)\n\n# add corpus to the learner\njulia> add!(bper, \"./test/data/corpus.en\")\nDict{String, Int64} with 4244 entries:\n  \"Ä exceedingly\" => 2\n  \"Ä queue\"       => 3\n  \"Ä fell\"        => 2\n  \"Ä local\"       => 3\n  \"Ä aspiration\"  => 1\n  \"Ä advancement\" => 1\n  \"Ä entrusts\"    => 1\n  \"Ä had\"         => 31\n  \"Ä retained\"    => 1\n  \"Ä cowdery\"     => 1\n  \"Ä nicer\"       => 1\n  \"Ä mission\"     => 1\n  \"Ä time\"        => 39\n  \"Ä executed\"    => 1\n  \"rather\"       => 1\n  \"Ä safely\"      => 3\n  \"Ä makes\"       => 1\n  \"Ä reach\"       => 1\n  \"these\"        => 2\n  \"Ä enough\"      => 2\n  \"Ä spot\"        => 3\n  \"Ä users\"       => 22\n  \"Ä pasta\"       => 1\n  \"Ä mode\"        => 1\n  \"Ä largely\"     => 2\n  \"Ä main\"        => 7\n  â‹®              => â‹®\n\n\n# learn the bpe\njulia> learn!(bper)\nGenericBPE{String}(n_merge=1168, endsym=@w@, sepsym==, oldendsym=@w@, input_transform=gpt2_tokenizer, glossaries=BytePairEncoding.Glossary(Regex[r\"[0-9]\"]), codemap=BytePairEncoding.CodeMap{UInt8, UInt16}(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ']), normalizer=UtfNormalizer(1038))\n\n# test our learned bpe\njulia> mybpe(\" Is this a ðŸ˜º\")\n8-element Vector{String}:\n \"Ä is@w@\"\n \"Ä this@w@\"\n \"Ä a@w@\"\n \"Ä =\"\n \"Ã°=\"\n \"Å=\"\n \"Äº=\"\n \"Âº@w@\"\n\n# dump the learned bpe\njulia> emit(bper, \"./bpe.out\")\n\"./bpe.out\"\n\nshell> head -n10 bpe.out\n:#endsym:@w@\nÄ  t\nÄ t h\nÄ  a\nÄ  i\nÄ  o\nÄ  s\nÄ  ,@w@\nÄ th e@w@\ni n\n","category":"page"},{"location":"learn/","page":"Learning BPE","title":"Learning BPE","text":"You can also use vocab = BytePairEncoding.get_vocab(mybpe, file) to get the vocabularies parallelly from given files.  And then use add!(bper, vocab) to merge the vocabularies and learn!(bper) the BPE.","category":"page"},{"location":"encode/#Encode","page":"Encode","title":"Encode","text":"","category":"section"},{"location":"encode/","page":"Encode","title":"Encode","text":"The overall encoding procedure:","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"normalize the input text.\nseparate word in glossary.\ntransform input (like tokenization) but no on glossary words.\ncodemapping for all words.\nbpe on non-glossary words.\noutput transform (like vocabulary) on all words.","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"see Utilities for some helper type/function.","category":"page"},{"location":"encode/#Basic-Type","page":"Encode","title":"Basic Type","text":"","category":"section"},{"location":"encode/","page":"Encode","title":"Encode","text":"GenericBPE{T}(;sepsym=nothing, oldendsym = nothing, endsym = oldendsym,\n\tinput_transform = nothing, output_transform = nothing,\n\tmerging_rank = Dict{Tuple{T, T}, Int}(), cache = Dict{T, Vector{T}}(),\n\tglossaries = nothing, codemap = nothing,\n\tnormalizer = nothing) where T","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"This is the signature of GenericBPE{String}. where:","category":"page"},{"location":"encode/","page":"Encode","title":"Encode","text":"sepsym is the extra tag for non-end word. For example: If we split \"word\" into \"wo\" and \"rd\" and we set the sepsym = \"_\", then the result would be [\"wo_\", \"rd\"].\nendsym is the extra tag for end word. This is usually equal to the value of oldendsym. For example: If we split \"word\" into \"wo\" and \"rd\" and we set the endsym = \"/\", then the result would be [\"wo\", \"rd/\"].\noldendsym is the endsym read from a bpe file.  For example: subword-mnt use \"<w/>\" as the end tag and thus need to be set accordingly. We can also set endsym to different value to override the setting.\ninput_transform: the tokenizer.\noutput_transform: some postprocessing function (like the vocabulary).\nmerging_rank: the bpe data.\ncache: cache for bpe result.\nglossaries: see Glossary\ncodemap: see Codepoint Mapping/UnMapping\nnormalizer: see Unicode Normalization","category":"page"},{"location":"#BytePairEncoding.jl","page":"Home","title":"BytePairEncoding.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pure Julia implementation of the Byte Pair Encoding(BPE) method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The design is inspired by the original python package subword-nmt and the byte-level bpe use in openai-gpt2. BytePairEncoding.jl support different tokenize method(with the help of WordTokenizers.jl). You can simply use set the tokenizer and then Learn the BPE map with it.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add BytePairEncoding","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using BytePairEncoding, WordTokenizers\n\n# using the bpe from openai gpt\njulia> bpe = Bpe(Base.download(\"https://huggingface.co/openai-gpt/resolve/main/merges.txt\"))\nGenericBPE{String}(n_merge=40000, endsym=</w>, oldendsym=</w>, input_transform=tokenize)\n\n# reset the tokenize method to do lowercase before tokenization\njulia> bpe = GenericBPE(bpe; input_transform = tokenizeâˆ˜lowercase)\nGenericBPE{String}(n_merge=40000, endsym=</w>, oldendsym=</w>, input_transform=ComposedFunction{typeof(tokenize), typeof(lowercase)}(WordTokenizers.tokenize, lowercase))\n\n# segment the sentence\njulia> bpe(\"Peter Piper picked a peck of pickled peppers\")\n8-element Vector{String}:\n \"peter</w>\"\n \"piper</w>\"\n \"picked</w>\"\n \"a</w>\"\n \"peck</w>\"\n \"of</w>\"\n \"pickled</w>\"\n \"peppers</w>\"\n\n# using the byte level bpe from openai gpt2\njulia> bbpe = ByteLevelBPE(Base.download(\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\"))\nGenericBPE{String}(n_merge=50000, input_transform=gpt2_tokenizer, codemap=BytePairEncoding.CodeMap(StepRange{Char,\nInt64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ']))\n\n# segment the sentence\njulia> bbpe(\"This is a ðŸ˜º\")\n5-element Vector{String}:\n \"This\"\n \"Ä is\"\n \"Ä a\"\n \"Ä Ã°ÅÄº\"\n \"Âº\"\n\n# to see the origin input, set the output_transform method that unmap the codepoint\njulia> decoded_bbpe = GenericBPE(bbpe; output_transform = BytePairEncoding.UnMap(bbpe.codemap))\nGenericBPE{String}(n_merge=50000, input_transform=gpt2_tokenizer, output_transform=BytePairEncoding.UnMap(BytePairEncoding.CodeMap(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ'])), codemap=BytePairEncoding.CodeMap(StepRange{Char, Int64}['\\0':1:' ', '\\x7f':1:'Â ', '\\uad':1:'\\uad'], StepRange{Char, Int64}['Ä€':1:'Ä ', 'Ä¡':1:'Å‚', 'Åƒ':1:'Åƒ']))\n\njulia> decoded_bbpe(\"This is a ðŸ˜º\")\n5-element Vector{String}:\n \"This\"\n \" is\"\n \" a\"\n \" \\xf0\\x9f\\x98\"\n \"\\xba\"\n\njulia> join(ans)\n\"This is a ðŸ˜º\"\n","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"encode.md\",\n  \"learn.md\",\n  \"utils.md\"\n]","category":"page"},{"location":"utils/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"Here are docs about some functionality we migth during the encoding process.","category":"page"},{"location":"utils/#Unicode-Normalization","page":"Utilities","title":"Unicode Normalization","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"We provide BytePairEncoding.UtfNormalizer for unicode normalization. It simply wrapBase.Unicode.normalize as a callable object that perform the normalization on input text.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> norm = BytePairEncoding.UtfNormalizer(:NFKC_CF)\nUtfNormalizer(1038)\n\njulia> norm(\"i'M nOt LOWeRCaSE\")\n\"i'm not lowercase\"\n","category":"page"},{"location":"utils/#Codepoint-Mapping/UnMapping","page":"Utilities","title":"Codepoint Mapping/UnMapping","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"In the byte level bpe, we split the unicode string into UTF-8 bytes and map them into some fixed code range. We can do this with BytePairEncoding.CodeMap.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"For example, we want to map Char(0):Char(15) to 'a':'p' and Char(16) also to 'p'.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> cm = BytePairEncoding.CodeMap([((Char(0):Char(15)) , ('a':'p')), Char(16)=>'p'])\nBytePairEncoding.CodeMap{UInt8, UInt8}(StepRange{Char, Int64}['\\0':1:'\\x0f', '\\x10':1:'\\x10'], StepRange{Char, Int64}['a':1:'p', 'p':1:'p'])\n\n# mapping the codepoint. \n# this is equivalent to `cm(\"\\x00\\x01\\x0f\\x10\")`\njulia> BytePairEncoding.encode(cm, \"\\x00\\x01\\x0f\\x10\")\n\"abpp\"\n\n# unmap the codedpoint.\n# this is equvalent to `BytePairEncoding.UnMap(cm)(ans)`\njulia> BytePairEncoding.decode(cm, ans)\n\"\\0\\x01\\x0f\\x0f\"\n","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"note: Note\nYou might find that the decoded string and the origin input are different, because the code mapping is not bijective.  Therefore a good design of codemap is important, or you can just use BytePairEncoding.default_codemap() for the codemap used by openai gpt2.","category":"page"},{"location":"utils/#Glossary","page":"Utilities","title":"Glossary","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"Glossary is a list of Regex that specifing some text shouldn't be split by tokenizer or BPE.","category":"page"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"julia> gloss = BytePairEncoding.Glossary([r\"[0-9]\", \"New York\"])\nBytePairEncoding.Glossary(Regex[r\"[0-9]\", r\"New\\ York\"])\n\njulia> gloss(\"New York123\")\n4-element Vector{String}:\n \"New York\"\n \"1\"\n \"2\"\n \"3\"\n","category":"page"}]
}
